{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 1**\n",
        "\n",
        "The Python script demonstrates token healing, which is a technique used to correct misspelt or mistyped words in text. It utilises a dictionary of misspelt words and their corresponding correct spellings to perform the token healing process."
      ],
      "metadata": {
        "id": "iJLbJ1EPuOO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgSjqRIVoCtv",
        "outputId": "9eb0f9e8-e398-4fdc-9ae0-ecd66b1d9be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They don't know abot thier wrods\n",
            "Original Text:\n",
            "They don't know abot thier wrods\n",
            "\n",
            "Corrected Text:\n",
            "They don't know about their words\n",
            "\n",
            "Number of Tokens: 6\n",
            "Number of Tokens Corrected: 3\n",
            "Correction Rate: 50.00%\n",
            "\n",
            "Incorrect Tokens:\n",
            "Original: abot            Corrected: about          \n",
            "Original: thier           Corrected: their          \n",
            "Original: wrods           Corrected: words          \n",
            "\n",
            "Unique Incorrect Tokens: 3\n",
            "Unique Incorrect Tokens: {'thier', 'abot', 'wrods'}\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def load_dictionary(file_path):\n",
        "    \"\"\"\n",
        "    Loads the CSV file which has the data for both the misspelled and correctly spelled word\n",
        "    \"\"\"\n",
        "    dictionary = {}\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            misspelled_word = row['Misspelled Word'].strip()\n",
        "            correct_word = row['Correct Word'].strip()\n",
        "            dictionary[misspelled_word] = correct_word\n",
        "    return dictionary\n",
        "\n",
        "def perform_token_healing(text, dictionary):\n",
        "    \"\"\"\n",
        "    Perform token healing on the given input text using the provided dictionary and provides the details of corrected tokens.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        if token in dictionary:\n",
        "            corrected_tokens.append(dictionary[token])\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "    corrected_text = ' '.join(corrected_tokens)\n",
        "    return corrected_text\n",
        "\n",
        "\n",
        "def analyze_token_healing(original_text, corrected_text):\n",
        "    \"\"\"\n",
        "    Perform analysis of the token healing process.\n",
        "    Prints the original text, corrected text, and provides various statistics.\n",
        "    \"\"\"\n",
        "    original_tokens = original_text.split()\n",
        "    corrected_tokens = corrected_text.split()\n",
        "    num_tokens = len(original_tokens)\n",
        "    num_corrected_tokens = sum(1 for original, corrected in zip(original_tokens, corrected_tokens) if original != corrected)\n",
        "    correction_rate = (num_corrected_tokens / num_tokens) * 100\n",
        "\n",
        "    print(\"Original Text:\")\n",
        "    print(original_text)\n",
        "    print()\n",
        "    print(\"Corrected Text:\")\n",
        "    print(corrected_text)\n",
        "    print()\n",
        "    print(\"Number of Tokens:\", num_tokens)\n",
        "    print(\"Number of Tokens Corrected:\", num_corrected_tokens)\n",
        "    print(\"Correction Rate: {:.2f}%\".format(correction_rate))\n",
        "\n",
        "    # Additional analysis done\n",
        "    incorrect_token_indices = [i for i, (original, corrected) in enumerate(zip(original_tokens, corrected_tokens)) if original != corrected]\n",
        "    incorrect_tokens = [original_tokens[i] for i in incorrect_token_indices]\n",
        "    corrected_tokens = [corrected_tokens[i] for i in incorrect_token_indices]\n",
        "\n",
        "    print()\n",
        "    print(\"Incorrect Tokens:\")\n",
        "    for original, corrected in zip(incorrect_tokens, corrected_tokens):\n",
        "        print(\"Original: {:<15} Corrected: {:<15}\".format(original, corrected))\n",
        "\n",
        "    unique_incorrect_tokens = set(incorrect_tokens)\n",
        "    print()\n",
        "    print(\"Unique Incorrect Tokens:\", len(unique_incorrect_tokens))\n",
        "    print(\"Unique Incorrect Tokens:\", unique_incorrect_tokens)\n",
        "\n",
        "\n",
        "# the csv file path that has the dataset of the correct and misspelled words. \n",
        "csv_file_path = '/content/Book1 (1).csv'\n",
        "\n",
        "# Input\n",
        "input_text = input()\n",
        "dictionary = load_dictionary(csv_file_path)\n",
        "corrected_text = perform_token_healing(input_text, dictionary)\n",
        "analyze_token_healing(input_text, corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 2**\n",
        "\n",
        "This Python script demonstrates the concept of token healing using Spacy, a popular natural language processing library. Token healing is a technique used to correct misspelled words or tokens in text by replacing them with their correct spellings."
      ],
      "metadata": {
        "id": "0vW4dsWOxZ62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXRwvtuCt5fB",
        "outputId": "e507abd8-e01e-43ef-ba14-eec5cc545d15"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import spacy\n",
        "\n",
        "def load_dictionary(file_path):\n",
        "    \"\"\"\n",
        "    Load the misspelled words and their correct spellings from a CSV file.\n",
        "    Returns a dictionary where the keys are misspelled words and the values are their correct spellings.\n",
        "    \"\"\"\n",
        "    dictionary = {}\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            misspelled_word = row['Misspelled Word'].strip()\n",
        "            correct_word = row['Correct Word'].strip()\n",
        "            dictionary[misspelled_word] = correct_word\n",
        "    return dictionary\n",
        "\n",
        "def perform_token_healing(text, dictionary):\n",
        "    \"\"\"\n",
        "    Perform token healing on the given input text using the provided dictionary and spaCy.\n",
        "    Returns the text with corrected tokens.\n",
        "    \"\"\"\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    corrected_tokens = []\n",
        "    for token in doc:\n",
        "        if token.text in dictionary:\n",
        "            corrected_tokens.append(dictionary[token.text])\n",
        "        else:\n",
        "            corrected_tokens.append(token.text)\n",
        "    corrected_text = ' '.join(corrected_tokens)\n",
        "    return corrected_text\n",
        "\n",
        "\n",
        "def analyze_token_healing(original_text, corrected_text):\n",
        "    \"\"\"\n",
        "    Perform analysis of the token healing process.\n",
        "    Prints the original text, corrected text, and provides various statistics.\n",
        "    \"\"\"\n",
        "    original_tokens = original_text.split()\n",
        "    corrected_tokens = corrected_text.split()\n",
        "    num_tokens = len(original_tokens)\n",
        "    num_corrected_tokens = sum(1 for original, corrected in zip(original_tokens, corrected_tokens) if original != corrected)\n",
        "    correction_rate = (num_corrected_tokens / num_tokens) * 100\n",
        "\n",
        "    print(\"Original Text:\")\n",
        "    print(original_text)\n",
        "    print()\n",
        "    print(\"Corrected Text:\")\n",
        "    print(corrected_text)\n",
        "    print()\n",
        "    print(\"Number of Tokens:\", num_tokens)\n",
        "    print(\"Number of Tokens Corrected:\", num_corrected_tokens)\n",
        "    print(\"Correction Rate: {:.2f}%\".format(correction_rate))\n",
        "\n",
        "    # Additional analysis\n",
        "    incorrect_token_indices = [i for i, (original, corrected) in enumerate(zip(original_tokens, corrected_tokens)) if original != corrected]\n",
        "    incorrect_tokens = [original_tokens[i] for i in incorrect_token_indices]\n",
        "    corrected_tokens = [corrected_tokens[i] for i in incorrect_token_indices]\n",
        "\n",
        "    print()\n",
        "    print(\"Incorrect Tokens:\")\n",
        "    for original, corrected in zip(incorrect_tokens, corrected_tokens):\n",
        "        print(\"Original: {:<15} Corrected: {:<15}\".format(original, corrected))\n",
        "\n",
        "    unique_incorrect_tokens = set(incorrect_tokens)\n",
        "    print()\n",
        "    print(\"Unique Incorrect Tokens:\", len(unique_incorrect_tokens))\n",
        "    print(\"Unique Incorrect Tokens:\", unique_incorrect_tokens)\n",
        "\n",
        "\n",
        "# the csv file path that has the dataset of the correct and misspelled words.\n",
        "csv_file_path = '/content/Book1 (1).csv'\n",
        "\n",
        "# Example usage\n",
        "input_text = input()\n",
        "dictionary = load_dictionary(csv_file_path)\n",
        "corrected_text = perform_token_healing(input_text, dictionary)\n",
        "analyze_token_healing(input_text, corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwer-lCIPmNd",
        "outputId": "e2866d38-2cdf-4188-c3df-4097a8f11ac1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They don't know abot thier wrods\n",
            "Original Text:\n",
            "They don't know abot thier wrods\n",
            "\n",
            "Corrected Text:\n",
            "They do n't know about their words\n",
            "\n",
            "Number of Tokens: 6\n",
            "Number of Tokens Corrected: 5\n",
            "Correction Rate: 83.33%\n",
            "\n",
            "Incorrect Tokens:\n",
            "Original: don't           Corrected: do             \n",
            "Original: know            Corrected: n't            \n",
            "Original: abot            Corrected: know           \n",
            "Original: thier           Corrected: about          \n",
            "Original: wrods           Corrected: their          \n",
            "\n",
            "Unique Incorrect Tokens: 5\n",
            "Unique Incorrect Tokens: {\"don't\", 'know', 'thier', 'abot', 'wrods'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 3** (**Still in progress**)\n",
        "\n",
        "This Python script demonstrates the concept of token healing using the GPT-2 model from the Hugging Face transformers library. Token healing is a technique used to correct inconsistencies between the prompt and the generated text when using language models.\n"
      ],
      "metadata": {
        "id": "hp-Xd6gn07Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "def perform_token_healing(prompt, generated_text):\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
        "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "    generated_tokens = tokenizer.encode(generated_text, add_special_tokens=False)\n",
        "\n",
        "    # Remove the last token from the prompt tokens\n",
        "    prompt_tokens = prompt_tokens[:-1]\n",
        "\n",
        "    # Check if the first token of the generated tokens matches the last token of the prompt tokens\n",
        "    if generated_tokens[0] != prompt_tokens[-1]:\n",
        "        # Replace the first token of the generated tokens with the last token of the prompt tokens\n",
        "        generated_tokens[0] = prompt_tokens[-1]\n",
        "\n",
        "    # Decode the tokens back into text\n",
        "    corrected_text = tokenizer.decode(generated_tokens)\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "# Take user input for prompt and generated text\n",
        "prompt = input(\"Enter the prompt: \")\n",
        "generated_text = input(\"Enter the generated text: \")\n",
        "\n",
        "# Perform token healing\n",
        "corrected_text = perform_token_healing(prompt, generated_text)\n",
        "\n",
        "# Display the results\n",
        "print(\"Prompt: \", prompt)\n",
        "print(\"Generated Text: \", generated_text)\n",
        "print(\"Corrected Text: \", corrected_text)\n"
      ],
      "metadata": {
        "id": "YZ2zSCzeRFHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 4** (**Still in progress**)\n",
        "\n",
        "The code is trying to implement token healing using the PyEnchant library. \n",
        "\n",
        "The code performs token healing by checking each token in the input text against a dictionary of correctly spelled words. If a token is found to be misspelled, it suggests a correction and replaces the misspelled token with the suggested correction. The code aims to improve the accuracy and readability of the text by automatically correcting common spelling mistakes."
      ],
      "metadata": {
        "id": "1QTJ3uiQyCI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyenchant\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v24hWKnyHhG",
        "outputId": "673e9086-d8ae-4c76-f616-ddf2980ac8f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.10/dist-packages (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyenchant‑3.2.1‑cp39‑cp39‑win_amd64.whl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkSGcr8Sy1mf",
        "outputId": "63420f40-7066-4253-a803-e4c593a84252"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Requirement 'pyenchant‑3.2.1‑cp39‑cp39‑win_amd64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pyenchant‑3.2.1‑cp39‑cp39‑win_amd64.whl is not a valid wheel filename.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import enchant\n",
        "import re\n",
        "\n",
        "def perform_token_healing(text):\n",
        "    # Initialize the spell checker\n",
        "    spell_checker = enchant.Dict(\"en_US\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "\n",
        "    # Perform token healing\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        # Check if the token is a word\n",
        "        if token.isalpha():\n",
        "            # Check if the token is misspelled\n",
        "            if not spell_checker.check(token):\n",
        "                # Get suggested replacements for the misspelled token\n",
        "                suggestions = spell_checker.suggest(token)\n",
        "                if suggestions:\n",
        "                    corrected_token = suggestions[0]  # Choose the first suggestion as the correction\n",
        "                else:\n",
        "                    corrected_token = token  # Use the original token if no suggestions available\n",
        "            else:\n",
        "                corrected_token = token  # Use the original token if it is correctly spelled\n",
        "        else:\n",
        "            corrected_token = token  # Keep non-word tokens as is\n",
        "\n",
        "        corrected_tokens.append(corrected_token)\n",
        "\n",
        "    # Reconstruct the corrected text\n",
        "    corrected_text = ' '.join(corrected_tokens)\n",
        "    return corrected_text\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_text = input(\"Enter the text to perform token healing: \")\n",
        "corrected_text = perform_token_healing(input_text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(input_text)\n",
        "print()\n",
        "print(\"Corrected Text:\")\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "id": "17LFYHkeyBpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}